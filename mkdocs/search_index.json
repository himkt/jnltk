{
    "docs": [
        {
            "location": "/",
            "text": "JNLTK\n\n\nJNLTK is the library for Natural Language Processing for Japanese\n\n\nInstall\n\n\npip install jnltk",
            "title": "Home"
        },
        {
            "location": "/#jnltk",
            "text": "JNLTK is the library for Natural Language Processing for Japanese",
            "title": "JNLTK"
        },
        {
            "location": "/#install",
            "text": "pip install jnltk",
            "title": "Install"
        },
        {
            "location": "/tokenize/",
            "text": "Tokenize\n\n\nSimpleRuleSentenceTokenizer\n\n\nSimpleRuleSentenceTokenizer\u306f\u65e5\u672c\u8a9e\u3067\u8a18\u8ff0\u3055\u308c\u305f\u30c6\u30ad\u30b9\u30c8\u3092\u6587\u7ae0\u5358\u4f4d\u306b\u5206\u5272\u3057\u307e\u3059\uff0e\n\n\n\u300c\u3002\u300d\u300c\uff0e\u300d\u300c.\u300d\u3092\u6587\u3092\u533a\u5207\u308b\u5206\u5272\u5b50\u3068\u3057\u3066\u3044\u307e\u3059\uff0e\n\n\n\u305f\u3060\u3057\uff0c\u30d6\u30e9\u30b1\u30c3\u30c8\u306e\u4e2d\u306b\u51fa\u73fe\u3059\u308b\u5206\u5272\u5b50\u306f\u6587\u306e\u7d42\u7aef\u3092\u8868\u3055\u306a\u3044\u3068\u3044\u3046\u30eb\u30fc\u30eb\u3092\u8ffd\u52a0\u3057\u307e\u3059\uff0e\n\n\nfrom jnltk.tokenize import SimpleRuleSntenceTokenizer\n\ntokenizer = SimpleRuleSntenceTokenizer()\ntext = '\u300c\u541b\u306e\u540d\u306f\u3002\u300d\u3068\u3044\u3046\u6620\u753b\u304c\u3042\u308b\u3002\u5c90\u961c\u770c\u306e\u98db\u9a28\u304c\u8056\u5730\u3067\u3042\u308b\u3002'\n\nsentences = tokenizer.tokenize(text)\nprint(sentences) # => ['\u300c\u541b\u306e\u540d\u306f\u3002\u300d\u3068\u3044\u3046\u6620\u753b\u304c\u3042\u308b\u3002', '\u5c90\u961c\u770c\u306e\u98db\u9a28\u304c\u8056\u5730\u3067\u3042\u308b\u3002']",
            "title": "Tokenize"
        },
        {
            "location": "/tokenize/#tokenize",
            "text": "",
            "title": "Tokenize"
        },
        {
            "location": "/tokenize/#simplerulesentencetokenizer",
            "text": "SimpleRuleSentenceTokenizer\u306f\u65e5\u672c\u8a9e\u3067\u8a18\u8ff0\u3055\u308c\u305f\u30c6\u30ad\u30b9\u30c8\u3092\u6587\u7ae0\u5358\u4f4d\u306b\u5206\u5272\u3057\u307e\u3059\uff0e  \u300c\u3002\u300d\u300c\uff0e\u300d\u300c.\u300d\u3092\u6587\u3092\u533a\u5207\u308b\u5206\u5272\u5b50\u3068\u3057\u3066\u3044\u307e\u3059\uff0e  \u305f\u3060\u3057\uff0c\u30d6\u30e9\u30b1\u30c3\u30c8\u306e\u4e2d\u306b\u51fa\u73fe\u3059\u308b\u5206\u5272\u5b50\u306f\u6587\u306e\u7d42\u7aef\u3092\u8868\u3055\u306a\u3044\u3068\u3044\u3046\u30eb\u30fc\u30eb\u3092\u8ffd\u52a0\u3057\u307e\u3059\uff0e  from jnltk.tokenize import SimpleRuleSntenceTokenizer\n\ntokenizer = SimpleRuleSntenceTokenizer()\ntext = '\u300c\u541b\u306e\u540d\u306f\u3002\u300d\u3068\u3044\u3046\u6620\u753b\u304c\u3042\u308b\u3002\u5c90\u961c\u770c\u306e\u98db\u9a28\u304c\u8056\u5730\u3067\u3042\u308b\u3002'\n\nsentences = tokenizer.tokenize(text)\nprint(sentences) # => ['\u300c\u541b\u306e\u540d\u306f\u3002\u300d\u3068\u3044\u3046\u6620\u753b\u304c\u3042\u308b\u3002', '\u5c90\u961c\u770c\u306e\u98db\u9a28\u304c\u8056\u5730\u3067\u3042\u308b\u3002']",
            "title": "SimpleRuleSentenceTokenizer"
        }
    ]
}